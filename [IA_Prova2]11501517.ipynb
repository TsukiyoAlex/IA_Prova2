{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aluno: Aléxandros Augustus\n",
    "\n",
    "## 1 Classificação\n",
    "\n",
    "### 1.1 Análise da base de dados e pré-processamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importando ferramentas principais\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "%matplotlib inline\n",
    "\n",
    "#importando ferramentas adicionais\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import neighbors\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import confusion_matrix \n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn import tree\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.cluster import AgglomerativeClustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#carregando a base e visualizando-a\n",
    "data = pd.read_csv('train-titanic.csv')\n",
    "\n",
    "print(data.shape) #(linhas, colunas)\n",
    "data.head() #5 primeiras instâncias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Detalhamento das colunas extraído do [Kaggle](https://www.kaggle.com/c/titanic/data) e traduzido:<br>\n",
    "**Colunas:**<br>\n",
    "*PassengerId:* ID único do passageiro;<br>\n",
    "*Survived:* Sobreviveu (1) ou morreu (0);<br>\n",
    "*Pclass:* Classe do passageiro (1ª, 2ª, ou 3ª);<br>\n",
    "*Name:* Nome do passageiro;<br>\n",
    "*Sex:* Sexo do passageiro;<br>\n",
    "*Age:* Idade do passageiro;<br>\n",
    "*SibSp:* Número de irmãos/cônjugues a bordo do Titanic;<br>\n",
    "*Parch:* Número de pais/filhos a bordo do Titanic;<br>\n",
    "*Ticket:* Número do bilhete;<br>\n",
    "*Fare:* Valor pago pelo bilhete;<br>\n",
    "*Cabin:* Número da cabine<br>\n",
    "*Embarked:* Onde o passageiro embarcou no navio (C = Cherbourg, S = Southampton, Q = Queenstown)<br>\n",
    "<br>\n",
    "Para melhor classificação, serão eliminadas as colunas *PassengerId*, *Name* e *Ticket* pois apresentam características únicas aos passageiros que não contribuem para determinar sua sobrevivência. Além disso, é necessário converter os dados de *Sex* e *Embarked* em dados numéricos para que seja possível processá-los com o scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#eliminação de colunas com dados pessoais ou únicos para cada passageiro\n",
    "newdata = data.drop(columns=['PassengerId','Name','Ticket'])\n",
    "\n",
    "#conversão de dados categóricos em dados numéricos\n",
    "newdata['Sex'] = LabelEncoder().fit_transform(data['Sex'].astype('str'))\n",
    "newdata['Embarked'] = LabelEncoder().fit_transform(data['Embarked'].astype('str'))\n",
    "\n",
    "#visualização da nova base de dados\n",
    "print(newdata.shape)\n",
    "newdata.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "É possível observar que a nova base agora possui 9 colunas em vez de 12, porém o número de linhas foi inalterado.<br>\n",
    "A nova legenda para as colunas *Sex* e *Embarked* é a seguinte:<br>\n",
    "*Sex:* female = 0, male = 1<br>\n",
    "*Embarked:* C = 0, Q = 1, S = 2<br>\n",
    "<br>\n",
    "Para melhor acurácia é importante também verificar a existência de atributos incompletos e tratá-los adequadamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#verifica quantas linhas em cada coluna possuem atributos com valor nulo\n",
    "null_columns = newdata.columns[newdata.isnull().any()]\n",
    "newdata[null_columns].isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A partir do código anterior verifica-se que a coluna *Age* possui cerca de 19,87% linhas inválidas, e a coluna *Cabin* possui cerca de 77,10% linhas inválidas.<br>\n",
    "A estratégia que será adotada nesta situação será remover a coluna *Cabin*, pois mais de 70% desta não possui informação, a coluna *Age* será mantida como está, visto que menos de 30% da mesma se encontra comprometida.<br>\n",
    "Após tratar todas as colunas, verifica-se a matriz de correlação."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#removendo a coluna Cabin\n",
    "newdata = newdata.drop(columns=['Cabin'])\n",
    "\n",
    "#plotando a matriz de correlação\n",
    "corr = newdata.corr()\n",
    "corr.style.background_gradient(cmap='coolwarm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A partir das informações presentes na matriz de correlação, serão removidas todas as colunas cujo módulo de correlação com a coluna *Survived* é menor que 25%, ou seja, todas as colunas cuja correlação com a coluna *Survived* seja superior a -0,25 para valores negativos, ou inferior a 0,25 para valores positivos.<br>\n",
    "No caso serão removidas as colunas *Pclass*, *Age*, *SibSp*, *Parch* e *Embarked*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remoção de colunas com baixa correlação com o atributo de saída\n",
    "newdata = newdata.drop(columns=['Pclass','Age','SibSp','Parch','Embarked'])\n",
    "\n",
    "#visualização da nova base\n",
    "print(newdata.shape)\n",
    "newdata.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora que a base possui apenas as colunas necessárias para inferir a classe *Survived*, verificaremos a presença de possíveis *outliers* nos atributos de entrada e o balanceamento das classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#histograma das classes\n",
    "newdata.hist('Survived')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#histograma do atributo Sex\n",
    "newdata.hist('Sex')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#boxplot do atributo Fare\n",
    "newdata.boxplot('Fare')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Através das plotagens feitas, é possível concluir que:<br>\n",
    "As classes não estão perfeitamente balanceadas, a classe \"Morreu\" possui cerca de 500 instâncias, enquanto que a classe \"Sobreviveu\" possui cerca de 300 instâncias. Entretanto a diferença entre elas é pequena o suficiente para não apresentar um problema onde seria necessário remover instâncias de uma classe para que seja possível realizar predições mais precisas.<br>\n",
    "Não há *outliers* no atributo \"Sexo\".<br>\n",
    "O atributo \"Tarifa\" possui muitos *outliers*, porém, removeremos apenas aqueles muito mais distantes, cujo valor excede 300."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove linhas cujo valor da tarifa ultrapassa 300\n",
    "newdata = newdata.loc[newdata['Fare']<=300]\n",
    "\n",
    "#verifica o número de linhas e colunas da nova base\n",
    "newdata.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora a base possui apenas 888 linhas, em vez de 891, ou seja, 3 linhas foram excluídas.<br>\n",
    "Com isso a base está pronta para ser dividida em conjuntos de treinamento e teste. Tais conjuntos serão gerados de forma aleatória, portanto, seguirá em anexo os conjuntos utilizados na execução deste programa, já que cada vez que o mesmo for executado, conjuntos diferentes serão gerados, assim fazendo com que resultados diferentes sejam encontrados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Não execute esta célula a menos que queira gerar novos conjuntos de treinamento e teste\n",
    "\n",
    "#Divide a base em treinamento e teste com o conjunto de teste correspondente a 20% da base original\n",
    "train, test = train_test_split(newdata,test_size=0.2)\n",
    "\n",
    "#Exporta os conjuntos de treinamento e teste gerados como arquivos .csv\n",
    "#Comente esta linha caso não deseje exportar novos arquivos\n",
    "export_csv = train.to_csv('classificacao-train.csv')\n",
    "export_csv = test.to_csv('classificacao-test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Escolha dos métodos\n",
    "Para esta atividade, foram escolhidos os métodos KNN e SVM.<br>\n",
    "O método KNN foi escolhido pois é um método simples e de rápida execução que pode apresentar bons resultados apesar de sua simplicidade, já o método SVM foi escolhido pois é outro algoritmo simples de classificação voltado para casos em que deseja-se separar 2 classes, porém, utilizando uma técnica semelhante a agrupamentos para prever os resultados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Caso deseje replicar os resultos obtidos, execute os comandos desta célula\n",
    "train = pd.read_csv('classificacao-train.csv')\n",
    "test = pd.read_csv('classificacao-test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.1 Implementação do KNN\n",
    "Decidiu-se utilizar o método de KNN com parâmetro k=1 e k=3, escolhidos por serem os menores valores válidos de k."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#definindo os classificadores para k=1 e k=3\n",
    "knn1 = neighbors.KNeighborsClassifier(1)\n",
    "knn3 = neighbors.KNeighborsClassifier(3)\n",
    "\n",
    "#treinando os métodos e armazenando as predições em vetores\n",
    "knn1.fit(train,train['Survived'])\n",
    "result_knn1 = knn1.predict(test)\n",
    "\n",
    "knn3.fit(train,train['Survived'])\n",
    "result_knn3 = knn3.predict(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.2 Implementação do SVM\n",
    "Optou-se por utilizar o método de SVM com funções kernel \"linear\" e \"polinomal de grau 3\". A função linear foi escolhida por ser a mais simples do SVM, enquanto que a polinomial é quase tão simples quanto a linear, afinal a função linear é uma função polinomial de grau 1, o grau 3 foi escolhido por apresentar curvatura mais semelhante à liear que a polinomial de grau 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#definindo os classificadores para kernel linear e polinomial\n",
    "svmline = svm.SVC(gamma='auto',kernel='linear')\n",
    "svmpoly = svm.SVC(gamma='auto',kernel='poly')\n",
    "\n",
    "#treinando os métodos e armazenando as predições em vetores\n",
    "svmline.fit(train,train['Survived'])\n",
    "result_svmline = svmline.predict(test)\n",
    "\n",
    "svmpoly.fit(train,train['Survived'])\n",
    "result_svmpoly = svmpoly.predict(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Resultados\n",
    "Para esta atividade, considerou-se a classe negativa como sendo a classe \"Morreu\" (0) e a classe positiva como sendo a classe \"Sobreviveu\" (1)\n",
    "#### 1.3.1 Resultados KNN com k=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#geração de relatório dos testes\n",
    "cm = confusion_matrix(test['Survived'],result_knn1)\n",
    "print ('Matriz de Confusão :')\n",
    "print(cm,'\\n')\n",
    "print ('Métricas : ')\n",
    "print (classification_report(test['Survived'],result_knn1)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Verdadeiros Negativos:** 117<br>\n",
    "**Falsos Negativos:** 2<br>\n",
    "**Falsos Positivos:** 1<br>\n",
    "**Verdadeiros Positivos:** 58<br>\n",
    "**Acurácia:** 98%<br>\n",
    "**Precisão:** 97%<br>\n",
    "**Especificidade:** 98%<br>\n",
    "**Sensibilidade:** 98%<br>\n",
    "#### 1.3.2 Resultados KNN com k=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#geração de relatório dos testes\n",
    "cm = confusion_matrix(test['Survived'],result_knn3)\n",
    "print ('Matriz de Confusão :')\n",
    "print(cm,'\\n')\n",
    "print ('Métricas : ')\n",
    "print (classification_report(test['Survived'],result_knn3)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Verdadeiros Negativos:** 112<br>\n",
    "**Falsos Negativos:** 7<br>\n",
    "**Falsos Positivos:** 1<br>\n",
    "**Verdadeiros Positivos:** 58<br>\n",
    "**Acurácia:** 96%<br>\n",
    "**Precisão:** 89%<br>\n",
    "**Especificidade:** 94%<br>\n",
    "**Sensibilidade:** 98%<br>\n",
    "#### 1.3.3 Resultados SVM com função de kernel linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#geração de relatório dos testes\n",
    "cm = confusion_matrix(test['Survived'],result_svmline)\n",
    "print ('Matriz de Confusão :')\n",
    "print(cm,'\\n')\n",
    "print ('Métricas : ')\n",
    "print (classification_report(test['Survived'],result_svmline)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Verdadeiros Negativos:** 119<br>\n",
    "**Falsos Negativos:** 0<br>\n",
    "**Falsos Positivos:** 0<br>\n",
    "**Verdadeiros Positivos:** 59<br>\n",
    "**Acurácia:** 100%<br>\n",
    "**Precisão:** 100%<br>\n",
    "**Especificidade:** 100%<br>\n",
    "**Sensibilidade:** 100%<br>\n",
    "#### 1.3.4 Resultados SVM com função de kernel polinomial de grau 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#geração de relatório dos testes\n",
    "cm = confusion_matrix(test['Survived'],result_svmline)\n",
    "print ('Matriz de Confusão :')\n",
    "print(cm,'\\n')\n",
    "print ('Métricas : ')\n",
    "print (classification_report(test['Survived'],result_svmline)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Verdadeiros Negativos:** 119<br>\n",
    "**Falsos Negativos:** 0<br>\n",
    "**Falsos Positivos:** 0<br>\n",
    "**Verdadeiros Positivos:** 59<br>\n",
    "**Acurácia:** 100%<br>\n",
    "**Precisão:** 100%<br>\n",
    "**Especificidade:** 100%<br>\n",
    "**Sensibilidade:** 100%<br>\n",
    "#### 1.3.5 Discussão dos resultados obtidos\n",
    "Verifica-se que o algoritmo que apresentou os melhores resultados para este conjunto específico de treinamento e teste foi o SVM, tendo acurácia de 100% tanto para função kernel linear quanto para a função polinomial de grau 3.<br>\n",
    "A acurácia representa a porcentagem de instâncias classificadas corretamente dentre todas, por isso uma acurácia de 100% significa que todas as outras métricas também possuem valor 100%, já que o número de instâncias corretamente classificadas é igual ao número total de instâncias no universo, tanto considerando-se a classe positiva quanto a negativa.<br>\n",
    "A precisão representa a porcentagem de instâncias corretamente classificadas positivas dentre todas as instâncias classificadas positivas.<br>\n",
    "A especificidade representa a porcentagem de instâncias corretamente classificadas negativas dentre todas as instâncias que pertencem à classe negativa. No relatório de classificação é representada pelo \"recall\" da classe negativa.<br>\n",
    "A sensibilidade representa a porcentagem de instâncias corretamente classificadas positivas dentre todas as instâncias que pertencem à classe positiva. No relatório de classificação é representada pelo \"recall\" da classe positiva.<br>\n",
    "Outro ponto importante a ser levantado é o fato do algortimo KNN com k=3 ter obtido uma acurácia menor que KNN com k=1. Uma possível explicação para tal fenômeno é que as instâncias que foram corretamente classificadas para k=1 e incorretamente classificadas para k=3 estavam muito próximas da fronteira de decisão, e ao ser ampliado o número de vizinhos verificados para classificá-las, foram encontrados mais vizinhos da classe incorreta que da classe correta, porém o vizinho mais próximo de tais instâncias ainda era aquele da classe correta.\n",
    "## 2 Regressão\n",
    "### 2.1 Análise da base de dados e pré-processamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#carregando uma nova base e visualizando-a\n",
    "data = pd.read_csv('train.csv')\n",
    "\n",
    "print(data.shape) #(linhas, colunas)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "É possível notar que há muitas colunas na base, e muitas delas possuem atributos não-numéricos, por isso vamos discretizar a base e em seguida analizar sua matriz de correlação para eliminar atributos que tenham módulo de correlação com a coluna *SalePrice* - que representa a saída que desejamos prever - inferior a 30%.<br>\n",
    "Para selecionar as colunas que necessitam de discretização, verificou-se no [Kaggle](https://www.kaggle.com/c/house-prices-advanced-regression-techniques/data) as colunas com valores do tipo *string*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#inicialização de uma nova base igual à base de dados original\n",
    "newdata = data\n",
    "\n",
    "#discretização de colunas\n",
    "newdata['MSZoning'] = LabelEncoder().fit_transform(newdata['MSZoning'].astype('str'))\n",
    "newdata['LotFrontage'] = LabelEncoder().fit_transform(newdata['LotFrontage'].astype('str'))\n",
    "newdata['Street'] = LabelEncoder().fit_transform(newdata['Street'].astype('str'))\n",
    "newdata['Alley'] = LabelEncoder().fit_transform(newdata['Alley'].astype('str'))\n",
    "newdata['LotShape'] = LabelEncoder().fit_transform(newdata['LotShape'].astype('str'))\n",
    "newdata['LandContour'] = LabelEncoder().fit_transform(newdata['LandContour'].astype('str'))\n",
    "newdata['Utilities'] = LabelEncoder().fit_transform(newdata['Utilities'].astype('str'))\n",
    "newdata['LotConfig'] = LabelEncoder().fit_transform(newdata['LotConfig'].astype('str'))\n",
    "newdata['LandSlope'] = LabelEncoder().fit_transform(newdata['LandSlope'].astype('str'))\n",
    "newdata['Neighborhood'] = LabelEncoder().fit_transform(newdata['Neighborhood'].astype('str'))\n",
    "newdata['Condition1'] = LabelEncoder().fit_transform(newdata['Condition1'].astype('str'))\n",
    "newdata['Condition2'] = LabelEncoder().fit_transform(newdata['Condition2'].astype('str'))\n",
    "newdata['BldgType'] = LabelEncoder().fit_transform(newdata['BldgType'].astype('str'))\n",
    "newdata['HouseStyle'] = LabelEncoder().fit_transform(newdata['HouseStyle'].astype('str'))\n",
    "newdata['RoofStyle'] = LabelEncoder().fit_transform(newdata['RoofStyle'].astype('str'))\n",
    "newdata['RoofMatl'] = LabelEncoder().fit_transform(newdata['RoofMatl'].astype('str'))\n",
    "newdata['Exterior1st'] = LabelEncoder().fit_transform(newdata['Exterior1st'].astype('str'))\n",
    "newdata['Exterior2nd'] = LabelEncoder().fit_transform(newdata['Exterior2nd'].astype('str'))\n",
    "newdata['MasVnrType'] = LabelEncoder().fit_transform(newdata['MasVnrType'].astype('str'))\n",
    "newdata['MasVnrArea'] = LabelEncoder().fit_transform(newdata['MasVnrArea'].astype('str'))\n",
    "newdata['ExterQual'] = LabelEncoder().fit_transform(newdata['ExterQual'].astype('str'))\n",
    "newdata['ExterCond'] = LabelEncoder().fit_transform(newdata['ExterCond'].astype('str'))\n",
    "newdata['Foundation'] = LabelEncoder().fit_transform(newdata['Foundation'].astype('str'))\n",
    "newdata['BsmtQual'] = LabelEncoder().fit_transform(newdata['BsmtQual'].astype('str'))\n",
    "newdata['BsmtCond'] = LabelEncoder().fit_transform(newdata['BsmtCond'].astype('str'))\n",
    "newdata['BsmtExposure'] = LabelEncoder().fit_transform(newdata['BsmtExposure'].astype('str'))\n",
    "newdata['BsmtFinType1'] = LabelEncoder().fit_transform(newdata['BsmtFinType1'].astype('str'))\n",
    "newdata['BsmtFinType2'] = LabelEncoder().fit_transform(newdata['BsmtFinType2'].astype('str'))\n",
    "newdata['Heating'] = LabelEncoder().fit_transform(newdata['Heating'].astype('str'))\n",
    "newdata['HeatingQC'] = LabelEncoder().fit_transform(newdata['HeatingQC'].astype('str'))\n",
    "newdata['CentralAir'] = LabelEncoder().fit_transform(newdata['CentralAir'].astype('str'))\n",
    "newdata['Electrical'] = LabelEncoder().fit_transform(newdata['Electrical'].astype('str'))\n",
    "newdata['KitchenQual'] = LabelEncoder().fit_transform(newdata['KitchenQual'].astype('str'))\n",
    "newdata['Functional'] = LabelEncoder().fit_transform(newdata['Functional'].astype('str'))\n",
    "newdata['FireplaceQu'] = LabelEncoder().fit_transform(newdata['FireplaceQu'].astype('str'))\n",
    "newdata['GarageType'] = LabelEncoder().fit_transform(newdata['GarageType'].astype('str'))\n",
    "newdata['GarageYrBlt'] = LabelEncoder().fit_transform(newdata['GarageYrBlt'].astype('str'))\n",
    "newdata['GarageFinish'] = LabelEncoder().fit_transform(newdata['GarageFinish'].astype('str'))\n",
    "newdata['GarageQual'] = LabelEncoder().fit_transform(newdata['GarageQual'].astype('str'))\n",
    "newdata['GarageCond'] = LabelEncoder().fit_transform(newdata['GarageCond'].astype('str'))\n",
    "newdata['PavedDrive'] = LabelEncoder().fit_transform(newdata['PavedDrive'].astype('str'))\n",
    "newdata['PoolQC'] = LabelEncoder().fit_transform(newdata['PoolQC'].astype('str'))\n",
    "newdata['Fence'] = LabelEncoder().fit_transform(newdata['Fence'].astype('str'))\n",
    "newdata['MiscFeature'] = LabelEncoder().fit_transform(newdata['MiscFeature'].astype('str'))\n",
    "newdata['SaleType'] = LabelEncoder().fit_transform(newdata['SaleType'].astype('str'))\n",
    "newdata['SaleCondition'] = LabelEncoder().fit_transform(newdata['SaleCondition'].astype('str'))\n",
    "\n",
    "#visualização da nova base\n",
    "newdata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#geração da matriz de correlação\n",
    "corr = newdata.corr()\n",
    "corr.style.background_gradient(cmap='coolwarm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remoção de colunas\n",
    "newdata = newdata.drop(columns=['Id','MSSubClass','MSZoning','LotFrontage','LotArea','Street','Alley','LotShape',\n",
    "                               'LandContour','Utilities','LotConfig','LandSlope','Neighborhood','Condition1','Condition2',\n",
    "                               'BldgType','HouseStyle','OverallCond','RoofStyle','RoofMatl','Exterior1st','Exterior2nd',\n",
    "                               'MasVnrType','MasVnrArea','ExterQual','ExterCond','BsmtCond','BsmtFinType1','BsmtFinType2',\n",
    "                               'BsmtFinSF2','BsmtUnfSF','Heating','CentralAir','Electrical','LowQualFinSF','BsmtFullBath',\n",
    "                               'BsmtHalfBath','HalfBath','BedroomAbvGr','KitchenAbvGr','Functional','GarageQual',\n",
    "                                'GarageCond','PavedDrive','EnclosedPorch','3SsnPorch','ScreenPorch','PoolArea','PoolQC',\n",
    "                               'Fence','MiscFeature','MiscVal','MoSold','YrSold','SaleType','SaleCondition'])\n",
    "\n",
    "print(newdata.shape)\n",
    "newdata.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Em seguida, é recomendável verificar a existência de linhas com falta de dados ou dados inválidos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#apresenta número de instâncias com valores inválidos por coluna\n",
    "null_columns = newdata.columns[newdata.isnull().any()]\n",
    "newdata[null_columns].isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Constata-se que todas as instâncias presentes na nova base de dados possuem valores válidos em todas as suas colunas.<br>\n",
    "Vamos então plotar os boxplots de cada coluna e remover linhas com *outliers* muito distantes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newdata.boxplot('SalePrice')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove outliers mais distantes\n",
    "newdata = newdata.loc[newdata['SalePrice']<=700000]\n",
    "\n",
    "#exibe o novo tamanho da base\n",
    "newdata.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newdata.boxplot('OverallQual')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newdata.boxplot('YearBuilt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newdata.boxplot('YearRemodAdd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newdata.boxplot('Foundation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newdata.boxplot('BsmtQual')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newdata.boxplot('BsmtExposure')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newdata.boxplot('BsmtFinSF1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newdata = newdata.loc[newdata['BsmtFinSF1']<=5000]\n",
    "\n",
    "newdata.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newdata.boxplot('TotalBsmtSF')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newdata.boxplot('HeatingQC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newdata.boxplot('1stFlrSF')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newdata.boxplot('2ndFlrSF')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newdata.boxplot('GrLivArea')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newdata = newdata.loc[newdata['GrLivArea']<=4000]\n",
    "\n",
    "newdata.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newdata.boxplot('FullBath')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newdata.boxplot('KitchenQual')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newdata.boxplot('TotRmsAbvGrd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newdata = newdata.loc[newdata['TotRmsAbvGrd']<=12]\n",
    "\n",
    "newdata.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newdata.boxplot('Fireplaces')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newdata.boxplot('FireplaceQu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newdata.boxplot('GarageType')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newdata.boxplot('GarageYrBlt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newdata.boxplot('GarageFinish')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newdata.boxplot('GarageCars')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newdata.boxplot('GarageArea')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newdata.boxplot('WoodDeckSF')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newdata = newdata.loc[newdata['WoodDeckSF']<=800]\n",
    "\n",
    "newdata.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newdata.boxplot('OpenPorchSF')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Temos então que o número de linhas da nova base de dados foi reduzido de 1460 para 1454, ou seja, 6 instâncias foram eliminadas por conterem valores muito distantes da maioria.<br>\n",
    "Para que possamos obter resultados ainda melhores, normalizaremos a base antes de aplicar os métodos de aprendizagem de máquina. Em seguida, a base normalizada será dividida em conjuntos de treinamento e teste para que possam ser aplicados os métodos de aprendizagem de máquina.<br>\n",
    "Os conjuntos de treinamento e teste que terão seus resultados analisados neste relatório seguirão em anexo com o mesmo, visto que tais conjuntos estão sendo gerados de forma aleatória, e a cada execução conjuntos diferentes são gerados de forma que resultados diferentes são obtidos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#limita os valores de todas as colunas em um alcance de 0 a 10\n",
    "normaldata=((newdata-newdata.min())/(newdata.max()-newdata.min()))*10\n",
    "\n",
    "#exibe a base normalizada\n",
    "normaldata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Não execute esta célula a menos que queira gerar novos conjuntos de treinamento e teste\n",
    "\n",
    "#Divide a base em treinamento e teste com o conjunto de teste correspondente a 20% da base original\n",
    "train, test = train_test_split(normaldata,test_size=0.2)\n",
    "\n",
    "#Exporta os conjuntos de treinamento e teste gerados como arquivos .csv\n",
    "#Comente esta linha caso não deseje exportar novos arquivos\n",
    "export_csv = train.to_csv('regressao-train.csv')\n",
    "export_csv = test.to_csv('regressao-test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Escolha dos métodos\n",
    "Para este problema, foram escolhidos os métodos KNN e árvore de decisão.<br>\n",
    "O método de KNN foi escolhido por ser um dos mais simples e, mesmo com sua simplicidade, ser capaz de gerar bons resultados. O valor de k escolhido foi k=2 pois assim o resultado previsto será a média das duas instâncias mais próximas das quais se tem conhecimento.<br>\n",
    "Já a árvore de decisão foi escolhida por ser um método que geralmente garante bons resultados mesmo em problemas de regressão, sendo capaz de gerar uma boa aproximação linear dos valores reais."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#caso deseje replicar os resultados obtidos, execute os comandos desta célula\n",
    "train = pd.read_csv('regressao-train.csv')\n",
    "test = pd.read_csv('regressao-test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.1 Implementação do KNN para regressão"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#definindo o classificador para k=2\n",
    "knnr = KNeighborsRegressor(n_neighbors=2)\n",
    "\n",
    "#treinando o método e armazenando as predições em um vetor\n",
    "knnr.fit(train,train['SalePrice'])\n",
    "result_knnr = knnr.predict(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.2 Implementação da árvore de decisão para regressão"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#definindo o classificador da árvore de decisão\n",
    "dtr = tree.DecisionTreeRegressor()\n",
    "\n",
    "#treinando o método e armazenando as predições em um vetor\n",
    "dtr.fit(train,train['SalePrice'])\n",
    "result_dtr = dtr.predict(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Resultados\n",
    "Para análise dos resultados utilizou-se como métricas as taxas de erro médio absoluta e quadrática.<br>\n",
    "A taxa de erro médio absoluta é a média do somatório das diferenças dos valores reais das instâncias com os valores previstos para as mesmas.<br>\n",
    "Já a taxa de erro média quadrática é semelhante à absoluta, com o diferencial que ao invés de usar as o somatório das diferenças, utiliza-se o somatório das diferenças elevadas ao quadrado, dessa forma caso a diferença seja muito grande a taxa cresce em um ritmo exponencial.<br>\n",
    "O menor valor possível para estas taxas é 0, significando que 100% das instâncias foram previstas perfeitamente. Como a base de dados foi normalizada esses valores passam a ser proporcionais ao número máximo para o qual a base é normalizada. Ao normalizar a base para o valor 10 foi possível encontrar o menor valor possível para as taxas de erro na ordem de $10^{-1}$ para o método de KNN e na ordem de $10^{-2}$ para o erro médio absoluto da árvore de decisão e $10^{-3}$ para o erro médio quadrático da mesma.\n",
    "#### 2.3.1 Resultados KNN para regressão com k=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Taxa de Erro Médio Absoluto: ',mean_absolute_error(test['SalePrice'], result_knnr))\n",
    "print('Taxa de Erro Médio Quadrático: ',mean_squared_error(test['SalePrice'],result_knnr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.2 Resultados Árvore de Decisão para regressão"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Taxa de Erro Médio Absoluto: ',mean_absolute_error(test['SalePrice'], result_dtr))\n",
    "print('Taxa de Erro Médio Quadrático: ',mean_squared_error(test['SalePrice'],result_dtr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Clusterização\n",
    "### 3.1 Análise dos dados e preparação para uso de clusterização"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#carregando uma nova base e visualizando-a\n",
    "data = pd.read_csv('transfer.csv')\n",
    "\n",
    "print(data.shape) #(linhas, colunas)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pelo que se pode observar e pela descrição da base no [Kaggle](https://www.kaggle.com/thesiff/premierleague1819), sabe-se que a coluna *team* é a única que possui atributos não-numéricos, sendo assim vamos primeiramente gerar uma nova base com esse atributos discretizados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#criando nova base igual à original e discretizando-a\n",
    "newdata = data\n",
    "\n",
    "newdata['team'] = LabelEncoder().fit_transform(newdata['team'].astype('str'))\n",
    "\n",
    "#exibindo nova base\n",
    "newdata.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para utilizar os métodos de clusterização e plotá-los posteriormente, é ideal que a base tenha sua dimensão, ou seja, seu número de colunas, reduzido para 2. Como a coluna *team* identifica diferentes times, esta será mantida. Para determinar o que fazer com as demais colunas vamos plotar a matrize de correlação para verificar se existem colunas com forte correlação que podem ser removidas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = newdata.corr()\n",
    "corr.style.background_gradient(cmap='coolwarm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Da matriz de correlação tem-se que a coluna *Total* é a que mais possui correlação acima de 50% com as demais colunas, isto se deve ao fato da coluna *Total* ser o somatório dos valores das colunas *end_2019* a *end_2010*, por este motivo esta coluna será a coluna a ser mantida na base final."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removendo colunas desnecessárias\n",
    "newdata = newdata.drop(columns=['end_2019','end_2018','end_2017','end_2016','end_2015','end_2014','end_2013','end_2012',\n",
    "                               'end_2011','end_2010'])\n",
    "\n",
    "#visualizando a base final\n",
    "newdata.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora que temos a base final que será utilizada pelos algoritmos de clusterização e plotada, vamos gerar outra base semelhante a esta, mas com os valores da coluna *Total* para que possamos comparar os resultados da clusterização e plotagem em cada base."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cria e limita os valores da coluna Total a um alcance de 0 a 10 e exibe a base normalizada\n",
    "normaldata['team'] = newdata['team']\n",
    "normaldata['Total']=((newdata['Total']-newdata['Total'].min())/(newdata['Total'].max()-newdata['Total'].min()))*10\n",
    "\n",
    "normaldata.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para verificar como a normalização pode ter alterado a base, vamos gerar e comparar os boxplots da coluna *Total* em ambas as bases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newdata.boxplot('Total')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normaldata.boxplot('Total')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verifac-se então que a normalização apenas muda a escala do eixo y no boxplot, não alterando de nenhuma forma  ainformação dos dados, porém, tal mudança de escala pode aproximar mais certas instâncias, gerando diferentes agrupamentos para o mesmo método de clusterização dependendo de qual base seja usada. Isto será verificado ao final desta seção.\n",
    "### 3.2 Implementação dos métodos\n",
    "Apesar da proposta de utilizar valores de k iguais a 2, 5, 10 e 100, como não há mais que 20 instâncias qualquer valor de k acima de 20 é rejeitado pelos algoritmos, por isso os valores de k que serão utilizados são 2, 5 e 10.\n",
    "#### 3.2.1 K-means\n",
    "Para k=2 utilizou-se os valores 1, 10 e 100 para o número máximo de iterações, para os demais valores de k o número máximo de iterações é o padrão do scikit-learn (300)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#definindo clusterizadores para bases normalizada e não-normalizada\n",
    "km2_1 = KMeans(n_clusters=2,max_iter=1).fit(newdata)\n",
    "kmn2_1 = KMeans(n_clusters=2,max_iter=1).fit(normaldata)\n",
    "km2_10 = KMeans(n_clusters=2,max_iter=10).fit(newdata)\n",
    "kmn2_10 = KMeans(n_clusters=2,max_iter=10).fit(normaldata)\n",
    "km2_100 = KMeans(n_clusters=2,max_iter=100).fit(newdata)\n",
    "kmn2_100 = KMeans(n_clusters=2,max_iter=100).fit(normaldata)\n",
    "km5 = KMeans(n_clusters=5).fit(newdata)\n",
    "kmn5 = KMeans(n_clusters=5).fit(normaldata)\n",
    "km10 = KMeans(n_clusters=10).fit(newdata)\n",
    "kmn10 = KMeans(n_clusters=10).fit(normaldata)\n",
    "\n",
    "#criando funções para imprimir os resultados\n",
    "def result_km2_1():\n",
    "    print('K-means com 2 classes e máximo de 1 iteração:\\n')\n",
    "    print('Dados não-normalizados:\\n')\n",
    "    print('Classes: ',km2_1.labels_)\n",
    "    print('Centróides:\\n',km2_1.cluster_centers_)\n",
    "    print('\\n')\n",
    "    print('Dados normalizados:\\n')\n",
    "    print('Classes: ',kmn2_1.labels_)\n",
    "    print('Centróides:\\n',kmn2_1.cluster_centers_)\n",
    "\n",
    "def result_km2_10():\n",
    "    print('K-means com 2 classes e máximo de 10 iterações:\\n')\n",
    "    print('Dados não-normalizados:\\n')\n",
    "    print('Classes: ',km2_10.labels_)\n",
    "    print('Centróides:\\n',km2_10.cluster_centers_)\n",
    "    print('\\n')\n",
    "    print('Dados normalizados:\\n')\n",
    "    print('Classes: ',kmn2_10.labels_)\n",
    "    print('Centróides:\\n',kmn2_10.cluster_centers_)\n",
    "    \n",
    "def result_km2_100():\n",
    "    print('K-means com 2 classes e máximo de 100 iterações:\\n')\n",
    "    print('Dados não-normalizados:\\n')\n",
    "    print('Classes: ',km2_100.labels_)\n",
    "    print('Centróides:\\n',km2_100.cluster_centers_)\n",
    "    print('\\n')\n",
    "    print('Dados normalizados:\\n')\n",
    "    print('Classes: ',kmn2_100.labels_)\n",
    "    print('Centróides:\\n',kmn2_100.cluster_centers_)\n",
    "    \n",
    "def result_km5():\n",
    "    print('K-means com 5 classes:\\n')\n",
    "    print('Dados não-normalizados:\\n')\n",
    "    print('Classes: ',km5.labels_)\n",
    "    print('Centróides:\\n',km5.cluster_centers_)\n",
    "    print('\\n')\n",
    "    print('Dados normalizados:\\n')\n",
    "    print('Classes: ',kmn5.labels_)\n",
    "    print('Centróides:\\n',kmn5.cluster_centers_)\n",
    "    \n",
    "def result_km10():\n",
    "    print('K-means com 10 classes:\\n')\n",
    "    print('Dados não-normalizados:\\n')\n",
    "    print('Classes: ',km10.labels_)\n",
    "    print('Centróides:\\n',km10.cluster_centers_)\n",
    "    print('\\n')\n",
    "    print('Dados normalizados:\\n')\n",
    "    print('Classes: ',kmn10.labels_)\n",
    "    print('Centróides:\\n',kmn10.cluster_centers_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.2 Hierárquico\n",
    "Para k=2 utilizou-se os parâmetros de linkage *ward* (minimiza a variância dos clusters), *complete* (utiliza a maior distância), *average* (utiliza a distância média) e *single* (utiliza a menor distância). Para os demais valores de k o parâmetro utilizado foi o padrão do scikit-learn (*ward*). O parâmetro de linkage é o que determina a métrica utilizada pelo algoritmo para ajustar as classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#definindo clusterizadores para bases normalizada e não-normalizada\n",
    "h2_w = AgglomerativeClustering(n_clusters=2,linkage=\"ward\").fit(newdata)\n",
    "hn2_w = AgglomerativeClustering(n_clusters=2,linkage=\"ward\").fit(normaldata)\n",
    "h2_c = AgglomerativeClustering(n_clusters=2,linkage=\"complete\").fit(newdata)\n",
    "hn2_c = AgglomerativeClustering(n_clusters=2,linkage=\"complete\").fit(normaldata)\n",
    "h2_a = AgglomerativeClustering(n_clusters=2,linkage=\"average\").fit(newdata)\n",
    "hn2_a = AgglomerativeClustering(n_clusters=2,linkage=\"average\").fit(normaldata)\n",
    "h2_s = AgglomerativeClustering(n_clusters=2,linkage=\"single\").fit(newdata)\n",
    "hn2_s = AgglomerativeClustering(n_clusters=2,linkage=\"single\").fit(normaldata)\n",
    "h5 = AgglomerativeClustering(n_clusters=5).fit(newdata)\n",
    "hn5 = AgglomerativeClustering(n_clusters=5).fit(normaldata)\n",
    "h10 = AgglomerativeClustering(n_clusters=10).fit(newdata)\n",
    "hn10 = AgglomerativeClustering(n_clusters=10).fit(normaldata)\n",
    "\n",
    "#criando funções para imprimir os resultados\n",
    "def result_h2_w():\n",
    "    print('Hierárquico com 2 classes e variância:\\n')\n",
    "    print('Dados não-normalizados:\\n')\n",
    "    print('Classes: ',h2_w.labels_)\n",
    "    print('\\n')\n",
    "    print('Dados normalizados:\\n')\n",
    "    print('Classes: ',hn2_w.labels_)\n",
    "    \n",
    "def result_h2_c():\n",
    "    print('Hierárquico com 2 classes e maior distância:\\n')\n",
    "    print('Dados não-normalizados:\\n')\n",
    "    print('Classes: ',h2_c.labels_)\n",
    "    print('\\n')\n",
    "    print('Dados normalizados:\\n')\n",
    "    print('Classes: ',hn2_c.labels_)\n",
    "    \n",
    "def result_h2_a():\n",
    "    print('Hierárquico com 2 classes e distância média:\\n')\n",
    "    print('Dados não-normalizados:\\n')\n",
    "    print('Classes: ',h2_a.labels_)\n",
    "    print('\\n')\n",
    "    print('Dados normalizados:\\n')\n",
    "    print('Classes: ',hn2_a.labels_)\n",
    "    \n",
    "def result_h2_s():\n",
    "    print('Hierárquico com 2 classes e menor distância:\\n')\n",
    "    print('Dados não-normalizados:\\n')\n",
    "    print('Classes: ',h2_s.labels_)\n",
    "    print('\\n')\n",
    "    print('Dados normalizados:\\n')\n",
    "    print('Classes: ',hn2_s.labels_)\n",
    "    \n",
    "def result_h5():\n",
    "    print('Hierárquico com 5 classes:\\n')\n",
    "    print('Dados não-normalizados:\\n')\n",
    "    print('Classes: ',h5.labels_)\n",
    "    print('\\n')\n",
    "    print('Dados normalizados:\\n')\n",
    "    print('Classes: ',hn5.labels_)\n",
    "    \n",
    "def result_h10():\n",
    "    print('Hierárquico com 10 classes:\\n')\n",
    "    print('Dados não-normalizados:\\n')\n",
    "    print('Classes: ',h10.labels_)\n",
    "    print('\\n')\n",
    "    print('Dados normalizados:\\n')\n",
    "    print('Classes: ',hn10.labels_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Resultados\n",
    "Primeiro vamos imprimir os relatórios dos algoritmos de clusterização e verificar se normalizar a base teve algum impacto dentro de cada algoritmo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_km2_1()\n",
    "result_km2_10()\n",
    "result_km2_100()\n",
    "result_km5()\n",
    "result_km10()\n",
    "result_h2_w()\n",
    "result_h2_c()\n",
    "result_h2_a()\n",
    "result_h2_s()\n",
    "result_h5()\n",
    "result_h10()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "É possível perceber de imediato que a base normalizada gerou grupos diferentes da base não-normalizada, mesmo que a informação contida nas bases tenha sido a mesma como verificado previamente nos boxplots.<br>\n",
    "A seguir plotaremos os gráficos dos diferentes agrupamentos obtidos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title('K-means com 2 classes e máximo de 1 iteração não-normalizado')\n",
    "plt.scatter(newdata['team'],newdata['Total'],c=km2_1.labels_)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title('K-means com 2 classes e máximo de 1 iteração normalizado')\n",
    "plt.scatter(normaldata['team'],normaldata['Total'],c=kmn2_1.labels_)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title('K-means com 2 classes e máximo de 10 iterações não-normalizado')\n",
    "plt.scatter(newdata['team'],newdata['Total'],c=km2_10.labels_)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title('K-means com 2 classes e máximo de 10 iterações normalizado')\n",
    "plt.scatter(normaldata['team'],normaldata['Total'],c=kmn2_10.labels_)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title('K-means com 2 classes e máximo de 100 iterações não-normalizado')\n",
    "plt.scatter(newdata['team'],newdata['Total'],c=km2_100.labels_)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title('K-means com 2 classes e máximo de 100 iterações normalizado')\n",
    "plt.scatter(normaldata['team'],normaldata['Total'],c=kmn2_100.labels_)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title('K-means com 5 classes não-normalizado')\n",
    "plt.scatter(newdata['team'],newdata['Total'],c=km5.labels_)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title('K-means com 5 classes normalizado')\n",
    "plt.scatter(normaldata['team'],normaldata['Total'],c=kmn5.labels_)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title('K-means com 10 classes não-normalizado')\n",
    "plt.scatter(newdata['team'],newdata['Total'],c=km10.labels_)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title('K-means com 10 classes normalizado')\n",
    "plt.scatter(normaldata['team'],normaldata['Total'],c=kmn10.labels_)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title('Hierárquico com 2 classes e linkage ward não-normalizado')\n",
    "plt.scatter(newdata['team'],newdata['Total'],c=h2_w.labels_)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title('Hierárquico com 2 classes e linkage ward normalizado')\n",
    "plt.scatter(normaldata['team'],normaldata['Total'],c=hn2_w.labels_)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title('Hierárquico com 2 classes e linkage complete não-normalizado')\n",
    "plt.scatter(newdata['team'],newdata['Total'],c=h2_c.labels_)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title('Hierárquico com 2 classes e linkage complete normalizado')\n",
    "plt.scatter(normaldata['team'],normaldata['Total'],c=hn2_c.labels_)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title('Hierárquico com 2 classes e linkage average não-normalizado')\n",
    "plt.scatter(newdata['team'],newdata['Total'],c=h2_a.labels_)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title('Hierárquico com 2 classes e linkage average normalizado')\n",
    "plt.scatter(normaldata['team'],normaldata['Total'],c=hn2_a.labels_)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title('Hierárquico com 2 classes e linkage single não-normalizado')\n",
    "plt.scatter(newdata['team'],newdata['Total'],c=h2_s.labels_)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title('Hierárquico com 2 classes e linkage single normalizado')\n",
    "plt.scatter(normaldata['team'],normaldata['Total'],c=hn2_s.labels_)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title('Hierárquico com 5 classes não-normalizado')\n",
    "plt.scatter(newdata['team'],newdata['Total'],c=h5.labels_)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title('Hierárquico com 5 classes normalizado')\n",
    "plt.scatter(normaldata['team'],normaldata['Total'],c=hn5.labels_)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title('Hierárquico com 10 classes não-normalizado')\n",
    "plt.scatter(newdata['team'],newdata['Total'],c=h10.labels_)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title('Hierárquico com 10 classes normalizado')\n",
    "plt.scatter(normaldata['team'],normaldata['Total'],c=h10.labels_)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A partir dos gráficos acima é possível perceber que para k=2 o algoritmo K-means não apresentou muita mudança variando-se o número máximo de iterações, porém, a base normalizada e a base não normalizada apresentaram resultados diferentes em ambos algoritmos e para todos os valores de k. Segundo um julgamento visual é possível afirmar que para valores de k superiores a 2 a base normalizada obteve resultados que parecem descrever melhor os agrupamentos enquanto que a base não-normalizada gerou grupos muito espalhados.<br>\n",
    "Quanto ao algoritmo hierárquico para k=2, nota-se que não há grande distinção entre o linkage *ward* e *complete*, enquanto que os linkages *average* e *single* geraram resultados diferenciados.<br>\n",
    "A partir de uma análise visual, é possível dizer que a escolha entre K-means ou hierárquico, no caso desta base, não faz muita diferença contanto que o parâmetro de linkage usado seja *ward* ou *complete*. E os melhores valores de k seriam 2 ou 5, visto que k=10 gerou gráficos muito confusos com pouca distinção entre as classes, entretanto, no caso de desejar-se utilizar k=10, o K-means normalizado apresentou resultados mais claros que o hierárquico, visualmente. Além disso vale ressaltar que utilizar a base normalizada gerou grupos mais organizados ou com melhor distribuição das instâncias que a base não-normalizada.<br>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
